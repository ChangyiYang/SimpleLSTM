{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataPrepocessing import *\n",
    "from NeuralNets import *\n",
    "from TrainMethod import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data read and preprocessing\n",
    "data = read_csv_as_ndarray(\"data/data_PBMR_400.csv\", [0], [0])\n",
    "data = generate_histogram(data, 1.0e-06, 1.5e-04)\n",
    "data = standarlize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and test data\n",
    "training_data = ReactorData(data, sequence_length= 10, start_percent= 0, end_percent= 0.75)\n",
    "testing_data = ReactorData(data, sequence_length= 10, start_percent= 0.75, end_percent= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural nets\n",
    "\n",
    "input_dim = training_data[0][0].shape[1]\n",
    "\n",
    "\n",
    "\n",
    "# print(output_dim)\n",
    "\n",
    "# some adjustable hyper-parameters\n",
    "nets_hyper_parameters = {\n",
    "'input_dim' : input_dim,\n",
    "'output_dim' : 1,\n",
    "'hidden_dim' : 64,\n",
    "'lstm_nums_layer' : 2,\n",
    "'dropout' : 0.2 \n",
    "}\n",
    "\n",
    "\n",
    "model = SimpleLSTM(**nets_hyper_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(training_data[0][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is 0.1722836236290831 in epoch 0\n",
      "The loss is 0.006658728873724801 in epoch 20\n",
      "The loss is 0.003068829740980673 in epoch 40\n",
      "The loss is 0.002756815910472198 in epoch 60\n",
      "The loss is 0.1971625920111665 in epoch 80\n",
      "The loss is 0.07308697386927138 in epoch 100\n",
      "The loss is 0.0664293478569864 in epoch 120\n",
      "The loss is 0.010476885903146242 in epoch 140\n",
      "The loss is 0.07957978986543664 in epoch 160\n",
      "The loss is 0.020437943992923344 in epoch 180\n",
      "Training is finished, the final loss is 0.008614708579659864\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer_parameters = {\n",
    "    'params':model.parameters(),\n",
    "    'lr' : 0.01,\n",
    "    'weight_decay' : 0.01\n",
    "}\n",
    "\n",
    "train_parameters = {\n",
    "    'model' : model,\n",
    "    'training_data' : training_data,\n",
    "    'epoch_num': 200,\n",
    "    'batch_size' : 16,\n",
    "    'optimizer' : optim.Adam(**optimizer_parameters),\n",
    "    'loss_fn' : nn.MSELoss(),\n",
    "    'print_out_frequency': 20\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "model, _ = train(** train_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 150, got 1500",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\86182\\GitHub\\SimpleLSTM\\PebbleBedReactor.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/86182/GitHub/SimpleLSTM/PebbleBedReactor.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m parameters \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/86182/GitHub/SimpleLSTM/PebbleBedReactor.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m:[\u001b[39m0.01\u001b[39m, \u001b[39m0.001\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/86182/GitHub/SimpleLSTM/PebbleBedReactor.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmax_epochs\u001b[39m\u001b[39m'\u001b[39m:[\u001b[39m100\u001b[39m, \u001b[39m200\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/86182/GitHub/SimpleLSTM/PebbleBedReactor.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/86182/GitHub/SimpleLSTM/PebbleBedReactor.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m Search_parameters \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/86182/GitHub/SimpleLSTM/PebbleBedReactor.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mparam_grid\u001b[39m\u001b[39m'\u001b[39m:parameters,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/86182/GitHub/SimpleLSTM/PebbleBedReactor.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m# 'cv' : 5,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/86182/GitHub/SimpleLSTM/PebbleBedReactor.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39merror_score\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/86182/GitHub/SimpleLSTM/PebbleBedReactor.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/86182/GitHub/SimpleLSTM/PebbleBedReactor.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/86182/GitHub/SimpleLSTM/PebbleBedReactor.ipynb#W6sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m best_model \u001b[39m=\u001b[39m GridSearch(partial_module, Net_parameters, Search_parameters, training_data,)\n",
      "File \u001b[1;32mc:\\Users\\86182\\GitHub\\SimpleLSTM\\TrainMethod.py:56\u001b[0m, in \u001b[0;36mGridSearch\u001b[1;34m(model, Net_parameters, Search_parameters, dataset)\u001b[0m\n\u001b[0;32m     52\u001b[0m x , y, z \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mlabels\u001b[39m.\u001b[39mshape\n\u001b[0;32m     54\u001b[0m labels \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mlabels\u001b[39m.\u001b[39mreshape((x, y\u001b[39m*\u001b[39mz))\n\u001b[1;32m---> 56\u001b[0m gs\u001b[39m.\u001b[39;49mfit(data, labels)\n\u001b[0;32m     58\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mbest score: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(gs\u001b[39m.\u001b[39mbest_score_))\n\u001b[0;32m     60\u001b[0m \u001b[39mreturn\u001b[39;00m gs\u001b[39m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\skorch\\regressor.py:91\u001b[0m, in \u001b[0;36mNeuralNetRegressor.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39m\"\"\"See ``NeuralNet.fit``.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \n\u001b[0;32m     82\u001b[0m \u001b[39mIn contrast to ``NeuralNet.fit``, ``y`` is non-optional to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m \n\u001b[0;32m     87\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[39m# pylint: disable=useless-super-delegation\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[39m# this is actually a pylint bug:\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[39m# https://github.com/PyCQA/pylint/issues/1085\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(NeuralNetRegressor, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\skorch\\net.py:1215\u001b[0m, in \u001b[0;36mNeuralNet.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarm_start \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitialized_:\n\u001b[0;32m   1213\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitialize()\n\u001b[1;32m-> 1215\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartial_fit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m   1216\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\skorch\\net.py:1174\u001b[0m, in \u001b[0;36mNeuralNet.partial_fit\u001b[1;34m(self, X, y, classes, **fit_params)\u001b[0m\n\u001b[0;32m   1172\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\u001b[39m'\u001b[39m\u001b[39mon_train_begin\u001b[39m\u001b[39m'\u001b[39m, X\u001b[39m=\u001b[39mX, y\u001b[39m=\u001b[39my)\n\u001b[0;32m   1173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m   1175\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1176\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\skorch\\net.py:1087\u001b[0m, in \u001b[0;36mNeuralNet.fit_loop\u001b[1;34m(self, X, y, epochs, **fit_params)\u001b[0m\n\u001b[0;32m   1084\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m   1085\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\u001b[39m'\u001b[39m\u001b[39mon_epoch_begin\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mon_epoch_kwargs)\n\u001b[1;32m-> 1087\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_single_epoch(dataset_train, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1088\u001b[0m                           step_fn\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_step, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m   1090\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_single_epoch(dataset_valid, training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1091\u001b[0m                           step_fn\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidation_step, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m   1093\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\u001b[39m\"\u001b[39m\u001b[39mon_epoch_end\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mon_epoch_kwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\skorch\\net.py:1122\u001b[0m, in \u001b[0;36mNeuralNet.run_single_epoch\u001b[1;34m(self, dataset, training, prefix, step_fn, **fit_params)\u001b[0m\n\u001b[0;32m   1120\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(dataset, training\u001b[39m=\u001b[39mtraining):\n\u001b[0;32m   1121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\u001b[39m\"\u001b[39m\u001b[39mon_batch_begin\u001b[39m\u001b[39m\"\u001b[39m, batch\u001b[39m=\u001b[39mbatch, training\u001b[39m=\u001b[39mtraining)\n\u001b[1;32m-> 1122\u001b[0m     step \u001b[39m=\u001b[39m step_fn(batch, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m   1123\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mrecord_batch(prefix \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_loss\u001b[39m\u001b[39m\"\u001b[39m, step[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mitem())\n\u001b[0;32m   1124\u001b[0m     batch_size \u001b[39m=\u001b[39m (get_len(batch[\u001b[39m0\u001b[39m]) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(batch, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m))\n\u001b[0;32m   1125\u001b[0m                   \u001b[39melse\u001b[39;00m get_len(batch))\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\skorch\\net.py:1007\u001b[0m, in \u001b[0;36mNeuralNet.train_step\u001b[1;34m(self, batch, **fit_params)\u001b[0m\n\u001b[0;32m   1000\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\n\u001b[0;32m   1001\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mon_grad_computed\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1002\u001b[0m         named_parameters\u001b[39m=\u001b[39mTeeGenerator(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_all_learnable_params()),\n\u001b[0;32m   1003\u001b[0m         batch\u001b[39m=\u001b[39mbatch,\n\u001b[0;32m   1004\u001b[0m     )\n\u001b[0;32m   1005\u001b[0m     \u001b[39mreturn\u001b[39;00m step[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m-> 1007\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_optimizer(step_fn)\n\u001b[0;32m   1008\u001b[0m \u001b[39mreturn\u001b[39;00m step_accumulator\u001b[39m.\u001b[39mget_step()\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\skorch\\net.py:963\u001b[0m, in \u001b[0;36mNeuralNet._step_optimizer\u001b[1;34m(self, step_fn)\u001b[0m\n\u001b[0;32m    961\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m    962\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 963\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(step_fn)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\optim\\sgd.py:130\u001b[0m, in \u001b[0;36mSGD.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> 130\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    132\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[0;32m    133\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\skorch\\net.py:997\u001b[0m, in \u001b[0;36mNeuralNet.train_step.<locals>.step_fn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_fn\u001b[39m():\n\u001b[0;32m    996\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_zero_grad_optimizer()\n\u001b[1;32m--> 997\u001b[0m     step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_step_single(batch, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    998\u001b[0m     step_accumulator\u001b[39m.\u001b[39mstore_step(step)\n\u001b[0;32m   1000\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\n\u001b[0;32m   1001\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mon_grad_computed\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1002\u001b[0m         named_parameters\u001b[39m=\u001b[39mTeeGenerator(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_all_learnable_params()),\n\u001b[0;32m   1003\u001b[0m         batch\u001b[39m=\u001b[39mbatch,\n\u001b[0;32m   1004\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\skorch\\net.py:896\u001b[0m, in \u001b[0;36mNeuralNet.train_step_single\u001b[1;34m(self, batch, **fit_params)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_training(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    895\u001b[0m Xi, yi \u001b[39m=\u001b[39m unpack_data(batch)\n\u001b[1;32m--> 896\u001b[0m y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(Xi, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    897\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_loss(y_pred, yi, X\u001b[39m=\u001b[39mXi, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    898\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\skorch\\net.py:1359\u001b[0m, in \u001b[0;36mNeuralNet.infer\u001b[1;34m(self, x, **fit_params)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     x_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_x_and_fit_params(x, fit_params)\n\u001b[0;32m   1358\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mx_dict)\n\u001b[1;32m-> 1359\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_(x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\86182\\GitHub\\SimpleLSTM\\NeuralNets.py:33\u001b[0m, in \u001b[0;36mSimpleLSTM.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m     31\u001b[0m     \u001b[39m# print(input.shape)\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     hidden_state, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mLSTM(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m     35\u001b[0m     \u001b[39m# print(hidden_state.shape)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_state)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:772\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    768\u001b[0m     \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    769\u001b[0m     \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    770\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 772\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[0;32m    773\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    774\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[0;32m    775\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:697\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    693\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[0;32m    694\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    695\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    696\u001b[0m                        ):\n\u001b[1;32m--> 697\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[0;32m    698\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[0;32m    699\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[0] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[0;32m    701\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:210\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    208\u001b[0m             expected_input_dim, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n\u001b[0;32m    209\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m--> 210\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    211\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    212\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 150, got 1500"
     ]
    }
   ],
   "source": [
    "\n",
    "# some adjustable hyper-parameters\n",
    "nets_hyper_parameters = {\n",
    "'input_dim' : input_dim,\n",
    "'output_dim' : 1,\n",
    "}\n",
    "\n",
    "# fix the input_dim and output_dim\n",
    "model = SimpleLSTM(**nets_hyper_parameters)\n",
    "partial_module = partial(model.__init__, **nets_hyper_parameters)\n",
    "\n",
    "\n",
    "\n",
    "Net_parameters = {\n",
    "    'module' : SimpleLSTM,\n",
    "    'max_epochs' : 200,\n",
    "    'lr':1e-4,\n",
    "    # 'optimizer': optim.Adam,\n",
    "    'criterion' : nn.MSELoss,\n",
    "    'verbose':0,\n",
    "\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    'lr':[0.01, 0.001],\n",
    "    'max_epochs':[100, 200]\n",
    "}\n",
    "\n",
    "Search_parameters = {\n",
    "    'param_grid':parameters,\n",
    "    # 'cv' : 5,\n",
    "    'error_score': 'raise',\n",
    "\n",
    "}\n",
    "\n",
    "best_model = GridSearch(partial_module, Net_parameters, Search_parameters, training_data,)\n",
    "\n",
    "# svd expect a 2D data and labels\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
