{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xe7ZFAr0zLUl"
   },
   "outputs": [],
   "source": [
    "# this code is written by Changyi Yang used for DS Discovery program\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch. __version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the data preprocessing\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocessing(file_name):\n",
    "    data = pd.read_csv(file_name, skiprows=[0]) # skip the first line\n",
    "    data = data.iloc[:, 1:] # skip the first row\n",
    "\n",
    "    ss = StandardScaler()\n",
    "\n",
    "\n",
    "    data = ss.fit_transform(data)\n",
    "\n",
    "    return data, ss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hX4XY0--zLUn"
   },
   "outputs": [],
   "source": [
    "# define the dataset classes\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ReactorData(Dataset):\n",
    "    def __init__(self,data, sequence_length, start_percent = 0, end_percent = 1):\n",
    "        \n",
    "        \n",
    "        length = data.shape[0]\n",
    "        data = data[ int(length * start_percent)  : int(length * end_percent)]\n",
    "        \n",
    "        # print(data.shape)\n",
    "        \n",
    "        \n",
    "        self.labels = data[:, -1:]\n",
    "        self.data = data[:, 0:-1]\n",
    "        \n",
    "\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)//self.sequence_length\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        idx = idx * self.sequence_length\n",
    "        \n",
    "        return (torch.tensor(self.data[idx : idx+ self.sequence_length])).double(), \\\n",
    "    (torch.tensor(self.labels[idx : idx+ self.sequence_length])).double()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "p7MGcyurzLUn"
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "data, ss = preprocessing('binnedpebbles.csv')\n",
    "\n",
    "# load the data\n",
    "sequence_length = 10\n",
    "\n",
    "training_data = ReactorData(data, sequence_length= sequence_length, start_percent= 0, end_percent= 0.75)\n",
    "testing_data = ReactorData(data, sequence_length= sequence_length, start_percent= 0.75, end_percent= 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "AK4tjCHtzLUo"
   },
   "outputs": [],
   "source": [
    "# print(training_data[0][1])\n",
    "# print(training_data[0][0].shape)\n",
    "# print(training_data[0][1].shape)\n",
    "\n",
    "# print(len(training_data[0]))\n",
    "# print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "RyAnlsI4zLUo"
   },
   "outputs": [],
   "source": [
    "# define the neural nets\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, lstm_nums_layer, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        \n",
    "        self.LSTM = nn.LSTM(input_dim, hidden_dim, lstm_nums_layer, batch_first = True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.hidden_to_output = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # print(input.shape)\n",
    "        \n",
    "        hidden_state, _ = self.LSTM(input)\n",
    "        \n",
    "        # print(hidden_state.shape)\n",
    "        output = self.dropout(hidden_state)\n",
    "        output = self.hidden_to_output(output)\n",
    "        \n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-9d-4dATzLUo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "input_dim = training_data[0][0].shape[1]\n",
    "output_dim = training_data[0][1].shape[1]\n",
    "\n",
    "\n",
    "# print(output_dim)\n",
    "\n",
    "# some adjustable hyper-parameters\n",
    "\n",
    "hidden_dim = 64\n",
    "num_hidden_layers = 2\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "epoch_num = 128\n",
    "dropout = 0.2\n",
    "\n",
    "\n",
    "def train(hidden_dim, num_hidden_layers, batch_size, learning_rate, weight_decay, epoch_num, dropout):\n",
    "    model = SimpleLSTM(input_dim, hidden_dim, output_dim, num_hidden_layers, dropout)\n",
    "    model = model.double()\n",
    "\n",
    "    train_dataloader = DataLoader(training_data, batch_size = batch_size)\n",
    "\n",
    "\n",
    "    # the chosn loss function and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay= weight_decay)\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "    \n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            model.zero_grad()\n",
    "        \n",
    "            pred = model(X)\n",
    "        \n",
    "            # print(X.shape)\n",
    "            # print(y.shape)\n",
    "        \n",
    "            loss = loss_fn(pred, y)\n",
    "        \n",
    "            # backpropagation\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            loss = loss.item()\n",
    "        \n",
    "        #if epoch % 5 == 0:\n",
    "         #   print(\"The loss is {} in epoch {}\".format(loss ,epoch))\n",
    "    return model\n",
    "            \n",
    "\n",
    "#print(f\"The training is ended, the final loss is {loss}.\")\n",
    "print(\"Bye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testing_data):\n",
    "    X_test = (torch.tensor(testing_data.data)).double()\n",
    "    y_test = (torch.tensor(testing_data.labels)).double()\n",
    "\n",
    "    y_pred = model(X_test)\n",
    "    loss = nn.MSELoss()\n",
    "    print(f\"The test loss is {loss(y_pred, y_test).item()}\")\n",
    "    return loss(y_pred, y_test).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test loss is 0.03379000274163063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.03379000274163063"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model = train(hidden_dim, num_hidden_layers, batch_size, learning_rate, weight_decay, epoch_num, dropout)\n",
    "test(trained_model, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test loss is 0.05199353278384666\n",
      "The test loss is 0.0419791265380754\n",
      "The test loss is 0.035434186889278546\n",
      "The test loss is 0.021591525851350726\n",
      "The test loss is 0.028525334587811233\n",
      "The test loss is 0.02094623542162918\n",
      "The test loss is 0.025475065061123684\n",
      "The test loss is 0.025091435477892263\n",
      "The test loss is 0.04281886208003941\n",
      "The test loss is 0.02116086442419024\n",
      "The test loss is 0.019408211159897427\n",
      "The test loss is 0.029037997453465043\n",
      "The test loss is 0.040875772633668456\n",
      "The test loss is 0.02165992417703914\n",
      "The test loss is 0.026408493277056737\n",
      "The test loss is 0.022049433751107288\n",
      "The test loss is 0.03482279330283606\n",
      "The test loss is 0.02611996039947959\n",
      "The test loss is 0.03656699765206594\n",
      "The test loss is 0.026284137260116\n",
      "The test loss is 0.02555383873286713\n",
      "The test loss is 0.026174909483185793\n",
      "The test loss is 0.03350624450468945\n",
      "The test loss is 0.025921855027407896\n",
      "The test loss is 0.020523220371840154\n",
      "The test loss is 0.02797619402543317\n",
      "The test loss is 0.023926900931954206\n",
      "The test loss is 0.03067090584744924\n",
      "The test loss is 0.02238447963170494\n",
      "The test loss is 0.01879902025282488\n",
      "The test loss is 0.0343577996662949\n",
      "The test loss is 0.024579891439702555\n",
      "The test loss is 0.031638072111819314\n",
      "The test loss is 0.31023559107871707\n",
      "The test loss is 0.7545029586361943\n",
      "The test loss is 0.871977465116405\n",
      "The test loss is 0.06367349870178768\n",
      "The test loss is 0.1177067403700778\n",
      "The test loss is 0.6432869231580483\n",
      "The test loss is 1.1101327575308708\n",
      "The test loss is 0.09224090923848619\n",
      "The test loss is 0.15626631008184408\n",
      "The test loss is 0.8399650975709\n",
      "The test loss is 0.9612632686338566\n",
      "The test loss is 0.06648824373006901\n",
      "The test loss is 0.23522974964820798\n",
      "The test loss is 0.642922046862952\n",
      "The test loss is 0.6899906460346321\n",
      "The test loss is 0.8287603291737219\n",
      "The test loss is 0.9343862374771551\n",
      "The test loss is 1.2182754937806035\n",
      "The test loss is 1.2420274059923575\n",
      "The test loss is 1.1968591186696027\n",
      "The test loss is 0.9320538053154565\n",
      "The test loss is 1.1492804844580755\n",
      "The test loss is 1.3104744440355267\n",
      "The test loss is 1.3581689532719796\n",
      "The test loss is 1.4203714231815345\n",
      "The test loss is 1.2337079213534614\n",
      "The test loss is 0.9848265618661988\n",
      "The test loss is 1.0357814944683652\n",
      "The test loss is 1.1985997144184448\n",
      "The test loss is 1.0104750836853225\n",
      "The test loss is 1.4646749532542993\n",
      "The test loss is 0.05323330522936972\n",
      "The test loss is 0.06480379146238494\n",
      "The test loss is 0.02458221810418942\n",
      "The test loss is 0.02820929821948721\n",
      "The test loss is 0.1028072206204705\n",
      "The test loss is 0.028538091859155776\n",
      "The test loss is 0.02901688869535991\n",
      "The test loss is 0.019551455366793893\n",
      "The test loss is 0.041250441165656494\n",
      "The test loss is 0.022078418675365787\n",
      "The test loss is 0.05762255440439318\n",
      "The test loss is 0.02475834461718139\n",
      "The test loss is 0.04865961815865277\n",
      "The test loss is 0.026140102145758713\n",
      "The test loss is 0.05374472768887471\n",
      "The test loss is 0.026719624009664735\n",
      "The test loss is 0.034528190206277565\n",
      "The test loss is 0.035390545131098454\n",
      "The test loss is 0.03054224489516047\n",
      "The test loss is 0.02806931638613044\n",
      "The test loss is 0.022258547870204853\n",
      "The test loss is 0.03163977896838527\n",
      "The test loss is 0.028204568954567476\n",
      "The test loss is 0.02450091958448109\n",
      "The test loss is 0.02154155814188162\n",
      "The test loss is 0.03666632485957184\n",
      "The test loss is 0.0303363875388361\n",
      "The test loss is 0.0383625725501791\n",
      "The test loss is 0.034447666551923696\n",
      "The test loss is 0.03281822301964332\n",
      "The test loss is 0.027326405062861277\n",
      "The test loss is 0.039068541674357644\n",
      "The test loss is 0.07646790234350007\n",
      "The test loss is 0.12245867759246869\n",
      "The test loss is 0.7716366978760761\n",
      "The test loss is 0.8038038506186491\n",
      "The test loss is 0.04282326160584047\n",
      "The test loss is 0.22935059011741052\n",
      "The test loss is 0.6442468198736319\n",
      "The test loss is 0.979284444590046\n",
      "The test loss is 0.056501711033913486\n",
      "The test loss is 0.31177849690781917\n",
      "The test loss is 0.84764475441621\n",
      "The test loss is 0.7539876814928436\n",
      "The test loss is 0.035120641624216395\n",
      "The test loss is 0.279953217429871\n",
      "The test loss is 0.7321120240920853\n",
      "The test loss is 0.8336520469310501\n",
      "The test loss is 0.9780275105647773\n",
      "The test loss is 1.1148326268483173\n",
      "The test loss is 1.045736627511913\n",
      "The test loss is 0.9577364564787497\n",
      "The test loss is 0.8588969396536612\n",
      "The test loss is 0.9613188317882626\n",
      "The test loss is 1.1806085307387995\n",
      "The test loss is 0.8708110691402767\n",
      "The test loss is 0.9478177453435166\n",
      "The test loss is 1.2051824405256164\n",
      "The test loss is 1.1050470439794784\n",
      "The test loss is 1.0471066649096488\n",
      "The test loss is 1.4354380835438103\n",
      "The test loss is 1.1156613749210271\n",
      "The test loss is 1.0359238093595853\n",
      "The test loss is 1.1244474868509817\n",
      "The test loss is 0.052340188211337096\n",
      "The test loss is 0.0404615175772027\n",
      "The test loss is 0.02981938408218091\n",
      "The test loss is 0.04444042100153393\n",
      "The test loss is 0.04940986906703021\n",
      "The test loss is 0.029441514304018215\n",
      "The test loss is 0.03706880410961601\n",
      "The test loss is 0.043929685227000095\n",
      "The test loss is 0.050126200990812025\n",
      "The test loss is 0.04464902057175089\n",
      "The test loss is 0.04686849789114877\n",
      "The test loss is 0.03899913280150019\n",
      "The test loss is 0.19243345191369546\n",
      "The test loss is 0.026591597013446495\n",
      "The test loss is 0.034287289304932564\n",
      "The test loss is 0.0444608849600982\n",
      "The test loss is 0.02658899663268066\n",
      "The test loss is 0.031835724063239056\n",
      "The test loss is 0.03411981882794955\n",
      "The test loss is 0.04216844363633462\n",
      "The test loss is 0.044716263411876074\n",
      "The test loss is 0.03115075566677053\n",
      "The test loss is 0.04561985899533139\n",
      "The test loss is 0.04203559897383379\n",
      "The test loss is 0.04727915128691355\n",
      "The test loss is 0.04919089427705643\n",
      "The test loss is 0.03790434143861462\n",
      "The test loss is 0.0654511061142692\n",
      "The test loss is 0.04518872934859429\n",
      "The test loss is 0.049859137371539776\n",
      "The test loss is 0.03725084933184177\n",
      "The test loss is 0.021656906302698023\n",
      "The test loss is 0.0718344010544877\n",
      "The test loss is 0.2700678583200255\n",
      "The test loss is 0.8761213657737833\n",
      "The test loss is 1.1297000609986207\n",
      "The test loss is 0.05907638082759039\n",
      "The test loss is 0.2448857227764014\n",
      "The test loss is 0.9958236823680033\n",
      "The test loss is 1.1140474806721565\n",
      "The test loss is 0.05401181533750444\n",
      "The test loss is 0.42704782037879946\n",
      "The test loss is 0.8323408318704963\n",
      "The test loss is 0.7779381910783287\n",
      "The test loss is 0.04896005381939219\n",
      "The test loss is 0.2670368010033558\n",
      "The test loss is 0.872977840601558\n",
      "The test loss is 0.8502846711553372\n",
      "The test loss is 0.9040677576284102\n",
      "The test loss is 1.094619078525937\n",
      "The test loss is 1.0747494743609123\n",
      "The test loss is 1.266926474025559\n",
      "The test loss is 1.1050082115251079\n",
      "The test loss is 1.1815621014932907\n",
      "The test loss is 0.9922514135168471\n",
      "The test loss is 1.404104069908949\n",
      "The test loss is 1.196436944002366\n",
      "The test loss is 1.2671186287211784\n",
      "The test loss is 1.0133288376221565\n",
      "The test loss is 1.0029389917840523\n",
      "The test loss is 0.9832980796784274\n",
      "The test loss is 1.130856009434125\n",
      "The test loss is 0.942486833931724\n",
      "The test loss is 1.387675904120737\n",
      "The test loss is 0.11769985409548821\n",
      "The test loss is 0.02194113533497964\n",
      "The test loss is 0.06809340146744006\n",
      "The test loss is 0.025021363401835424\n",
      "The test loss is 0.07093872494906261\n",
      "The test loss is 0.02628586131536143\n",
      "The test loss is 0.026384445526528376\n",
      "The test loss is 0.02790738679560703\n",
      "The test loss is 0.06512330028844283\n",
      "The test loss is 0.02820365121573343\n",
      "The test loss is 0.11933183901790133\n",
      "The test loss is 0.019804688654891548\n",
      "The test loss is 0.024197085495671185\n",
      "The test loss is 0.02568004749129896\n",
      "The test loss is 0.026421032093772643\n",
      "The test loss is 0.028470366745934846\n",
      "The test loss is 0.019449272525972276\n",
      "The test loss is 0.022745177194724284\n",
      "The test loss is 0.021434167520275868\n",
      "The test loss is 0.023108889038906174\n",
      "The test loss is 0.01983049247341206\n",
      "The test loss is 0.025448748697481923\n",
      "The test loss is 0.019783565281318114\n",
      "The test loss is 0.022023464567650242\n",
      "The test loss is 0.019631621882741784\n",
      "The test loss is 0.02187111746924557\n",
      "The test loss is 0.023359945871889456\n",
      "The test loss is 0.018746990914716332\n",
      "The test loss is 0.024558063816677494\n",
      "The test loss is 0.025410744582573086\n",
      "The test loss is 0.01855902414030325\n",
      "The test loss is 0.021918290919621695\n",
      "The test loss is 0.03787226021495236\n",
      "The test loss is 0.04944033083820719\n",
      "The test loss is 0.041819606596836964\n",
      "The test loss is 0.3116512387493485\n",
      "The test loss is 0.0375699908665773\n",
      "The test loss is 0.05892873585692395\n",
      "The test loss is 0.12762100868023873\n",
      "The test loss is 0.38850091706429446\n",
      "The test loss is 0.02240251369235406\n",
      "The test loss is 0.04928767715208819\n",
      "The test loss is 0.056790526984727155\n",
      "The test loss is 0.29156268803409985\n",
      "The test loss is 0.022201139257062685\n",
      "The test loss is 0.06371565273357184\n",
      "The test loss is 0.06627568912542905\n",
      "The test loss is 0.28796313795479067\n",
      "The test loss is 1.240256059339661\n",
      "The test loss is 1.1190406974412168\n",
      "The test loss is 1.0381063626610945\n",
      "The test loss is 0.9544302991835545\n",
      "The test loss is 0.9389429673655629\n",
      "The test loss is 1.0354770074467015\n",
      "The test loss is 1.232803252539101\n",
      "The test loss is 1.0555316720474086\n",
      "The test loss is 0.9747343108005266\n",
      "The test loss is 1.2535855305825538\n",
      "The test loss is 0.923013084595171\n",
      "The test loss is 1.0288572080566125\n",
      "The test loss is 0.8930814381443017\n",
      "The test loss is 0.9958244460625816\n",
      "The test loss is 1.2196896085089575\n",
      "The test loss is 1.1852622145999496\n",
      "The test loss is 0.050001870053661204\n",
      "The test loss is 0.053729471235905456\n",
      "The test loss is 0.03549703274433094\n",
      "The test loss is 0.026431736374810413\n",
      "The test loss is 0.02183601880801361\n",
      "The test loss is 0.07024579632640109\n",
      "The test loss is 0.02944444567277492\n",
      "The test loss is 0.024067771301808396\n",
      "The test loss is 0.07075416117860264\n",
      "The test loss is 0.09321093257753472\n",
      "The test loss is 0.033713657731995826\n",
      "The test loss is 0.021027797907058016\n",
      "The test loss is 0.12652223584575462\n",
      "The test loss is 0.029685602704235443\n",
      "The test loss is 0.026888493650418947\n",
      "The test loss is 0.02768062318191013\n",
      "The test loss is 0.02196877604051743\n",
      "The test loss is 0.032213223898868645\n",
      "The test loss is 0.01750527239071105\n",
      "The test loss is 0.028667569477678997\n",
      "The test loss is 0.023585663673820884\n",
      "The test loss is 0.024101166427768834\n",
      "The test loss is 0.020606120546922742\n",
      "The test loss is 0.03372374793823479\n",
      "The test loss is 0.023722469888595346\n",
      "The test loss is 0.023358553924265852\n",
      "The test loss is 0.02033441919703054\n",
      "The test loss is 0.021357077810147954\n",
      "The test loss is 0.022405626717828666\n",
      "The test loss is 0.021486970368375498\n",
      "The test loss is 0.026679586000277134\n",
      "The test loss is 0.019621585279557176\n",
      "The test loss is 0.044835360850448404\n",
      "The test loss is 0.04906160703170602\n",
      "The test loss is 0.0631241238494888\n",
      "The test loss is 0.41408581902410624\n",
      "The test loss is 0.039455093091701574\n",
      "The test loss is 0.06495767924687168\n",
      "The test loss is 0.05030421434540907\n",
      "The test loss is 0.29450706710799945\n",
      "The test loss is 0.033115085614345896\n",
      "The test loss is 0.05676099730792196\n",
      "The test loss is 0.10293375091752285\n",
      "The test loss is 0.42697403518143057\n",
      "The test loss is 0.03599178593795074\n",
      "The test loss is 0.04259292065578464\n",
      "The test loss is 0.05710310356086516\n",
      "The test loss is 0.4259443092273814\n",
      "The test loss is 0.9326558956425666\n",
      "The test loss is 1.20786446000397\n",
      "The test loss is 0.9826130448448668\n",
      "The test loss is 1.055275557800662\n",
      "The test loss is 0.885140324900915\n",
      "The test loss is 0.9671059451675751\n",
      "The test loss is 0.9752517006820702\n",
      "The test loss is 0.9628880260161741\n",
      "The test loss is 0.9574887765614754\n",
      "The test loss is 1.1643449170470916\n",
      "The test loss is 1.3204195196570017\n",
      "The test loss is 1.1660543327029353\n",
      "The test loss is 1.053729062253129\n",
      "The test loss is 1.0481809290055017\n",
      "The test loss is 1.2718221828082938\n",
      "The test loss is 1.1563412337435284\n",
      "The test loss is 0.12429675973066914\n",
      "The test loss is 0.026137755554775304\n",
      "The test loss is 0.07177471232149907\n",
      "The test loss is 0.02385340362175328\n",
      "The test loss is 0.12496681237387942\n",
      "The test loss is 0.08146563702027325\n",
      "The test loss is 0.047279438997862744\n",
      "The test loss is 0.02067596032085964\n",
      "The test loss is 0.03017400259013076\n",
      "The test loss is 0.0626953780343499\n",
      "The test loss is 0.047267610369887705\n",
      "The test loss is 0.04488282767890025\n",
      "The test loss is 0.027381538058663862\n",
      "The test loss is 0.034651389876140935\n",
      "The test loss is 0.04471337977101883\n",
      "The test loss is 0.03494924411965366\n",
      "The test loss is 0.015664752091945486\n",
      "The test loss is 0.022084776576332912\n",
      "The test loss is 0.02979809978377404\n",
      "The test loss is 0.03205554232230847\n",
      "The test loss is 0.026845777773531546\n",
      "The test loss is 0.02371074937561748\n",
      "The test loss is 0.032616619405906225\n",
      "The test loss is 0.022668381908735664\n",
      "The test loss is 0.03137777343340795\n",
      "The test loss is 0.02177962323530851\n",
      "The test loss is 0.025887781172780473\n",
      "The test loss is 0.035746327411380875\n",
      "The test loss is 0.02442510497381941\n",
      "The test loss is 0.026464133400315157\n",
      "The test loss is 0.03555194707554906\n",
      "The test loss is 0.02971736657640921\n",
      "The test loss is 0.029674044042203945\n",
      "The test loss is 0.08212134442593609\n",
      "The test loss is 0.04424517298439575\n",
      "The test loss is 0.40432073014841075\n",
      "The test loss is 0.0377284767007156\n",
      "The test loss is 0.1129398240844013\n",
      "The test loss is 0.06347117530674493\n",
      "The test loss is 0.24099085018946184\n",
      "The test loss is 0.030829367968623088\n",
      "The test loss is 0.07414434494612636\n",
      "The test loss is 0.13507734340161073\n",
      "The test loss is 0.4157715008949539\n",
      "The test loss is 0.04322308582212714\n",
      "The test loss is 0.06164308346779435\n",
      "The test loss is 0.07832149020845525\n",
      "The test loss is 0.40920607242094875\n",
      "The test loss is 1.0923285957472286\n",
      "The test loss is 1.008262605635428\n",
      "The test loss is 0.9939529949241895\n",
      "The test loss is 1.2769658882295116\n",
      "The test loss is 0.8484259981977884\n",
      "The test loss is 0.8899812407193256\n",
      "The test loss is 1.1102506597349884\n",
      "The test loss is 1.2061233621565037\n",
      "The test loss is 1.2308994012007701\n",
      "The test loss is 0.9858735726094668\n",
      "The test loss is 0.99839119667979\n",
      "The test loss is 1.0338603425011694\n",
      "The test loss is 0.9534302991660345\n",
      "The test loss is 1.1585060724305944\n",
      "The test loss is 0.9447014622809715\n",
      "The test loss is 1.0970730489577538\n",
      "The test loss is 0.1560250382405183\n",
      "The test loss is 0.09925356069246477\n",
      "The test loss is 0.0903977756727399\n",
      "The test loss is 0.025089139150472478\n",
      "The test loss is 0.04586841735567854\n",
      "The test loss is 0.16464291180683105\n",
      "The test loss is 0.024797630292917597\n",
      "The test loss is 0.02528553380754079\n",
      "The test loss is 0.03130575391348986\n",
      "The test loss is 0.06863218977925249\n",
      "The test loss is 0.03859499636113001\n",
      "The test loss is 0.026552430552051774\n",
      "The test loss is 0.1335519204279661\n",
      "The test loss is 0.017750057136450354\n",
      "The test loss is 0.03408440451953047\n",
      "The test loss is 0.028232454370135773\n",
      "The test loss is 0.019152725034712877\n",
      "The test loss is 0.02097484308137207\n",
      "The test loss is 0.016294220450536637\n",
      "The test loss is 0.017862523710401486\n",
      "The test loss is 0.020792762810267044\n",
      "The test loss is 0.02351745888386686\n",
      "The test loss is 0.017084073691237125\n",
      "The test loss is 0.018927345119708346\n",
      "The test loss is 0.03047406894503131\n",
      "The test loss is 0.017668154079535724\n",
      "The test loss is 0.014991668915392399\n",
      "The test loss is 0.014685474129954107\n",
      "The test loss is 0.021255848363463916\n",
      "The test loss is 0.017914311489182957\n",
      "The test loss is 0.01791766467304399\n",
      "The test loss is 0.017806007250431716\n",
      "The test loss is 0.01589741622879248\n",
      "The test loss is 0.019593327384628965\n",
      "The test loss is 0.04057107770907258\n",
      "The test loss is 0.07302696716642906\n",
      "The test loss is 0.017569078273331158\n",
      "The test loss is 0.018684638931800276\n",
      "The test loss is 0.037052069344438304\n",
      "The test loss is 0.08464008937825586\n",
      "The test loss is 0.02268798315721427\n",
      "The test loss is 0.020607799281205135\n",
      "The test loss is 0.03581476764771051\n",
      "The test loss is 0.052349195486779576\n",
      "The test loss is 0.027764358316468427\n",
      "The test loss is 0.029478256989378806\n",
      "The test loss is 0.05493949029622958\n",
      "The test loss is 0.05264975463850601\n",
      "The test loss is 0.6614873168270642\n",
      "The test loss is 1.083672630869795\n",
      "The test loss is 1.067104341018317\n",
      "The test loss is 1.129772873093149\n",
      "The test loss is 0.7377210861307208\n",
      "The test loss is 1.1094243168763147\n",
      "The test loss is 1.0365075673051403\n",
      "The test loss is 0.9206245234812007\n",
      "The test loss is 0.6399206419472657\n",
      "The test loss is 1.0709252474913875\n",
      "The test loss is 1.0621773763806102\n",
      "The test loss is 1.116040413662361\n",
      "The test loss is 0.6405042175065427\n",
      "The test loss is 1.004002043126521\n",
      "The test loss is 1.1932647429057195\n",
      "The test loss is 1.0274154674812188\n",
      "The test loss is 0.10047417828811017\n",
      "The test loss is 0.02824127144225008\n",
      "The test loss is 0.05930400786319851\n",
      "The test loss is 0.02584439198216305\n",
      "The test loss is 0.056439775699021255\n",
      "The test loss is 0.07402491641285955\n",
      "The test loss is 0.06557425095595988\n",
      "The test loss is 0.023633343488135727\n",
      "The test loss is 0.3005666014396405\n",
      "The test loss is 0.03382960447827804\n",
      "The test loss is 0.022915657181498485\n",
      "The test loss is 0.031735615155738514\n",
      "The test loss is 0.05521742265058324\n",
      "The test loss is 0.026581004875891184\n",
      "The test loss is 0.023192019807709085\n",
      "The test loss is 0.026380870833521804\n",
      "The test loss is 0.01996578567320467\n",
      "The test loss is 0.020449637833259825\n",
      "The test loss is 0.019574254343017364\n",
      "The test loss is 0.015465934255997068\n",
      "The test loss is 0.0176272110100141\n",
      "The test loss is 0.024456306115323213\n",
      "The test loss is 0.019240907942640686\n",
      "The test loss is 0.018004211682346546\n",
      "The test loss is 0.02154495161202545\n",
      "The test loss is 0.020219423450310085\n",
      "The test loss is 0.018692374095549524\n",
      "The test loss is 0.020031566852474578\n",
      "The test loss is 0.02464129666490484\n",
      "The test loss is 0.01998080296709712\n",
      "The test loss is 0.01901713406349314\n",
      "The test loss is 0.01657934195224474\n",
      "The test loss is 0.027964896953681963\n",
      "The test loss is 0.024079201526674095\n",
      "The test loss is 0.0325387211045645\n",
      "The test loss is 0.05115845589844137\n",
      "The test loss is 0.018061328052297678\n",
      "The test loss is 0.01705734111329641\n",
      "The test loss is 0.04230659016997439\n",
      "The test loss is 0.08631432703500044\n",
      "The test loss is 0.01963650224165448\n",
      "The test loss is 0.02284512189737085\n",
      "The test loss is 0.03874250349632344\n",
      "The test loss is 0.06253994340672549\n",
      "The test loss is 0.019077488742875726\n",
      "The test loss is 0.02416683361512095\n",
      "The test loss is 0.03879119512992484\n",
      "The test loss is 0.03683141555786344\n",
      "The test loss is 0.707866629520089\n",
      "The test loss is 1.0158277248648435\n",
      "The test loss is 1.182189164778592\n",
      "The test loss is 1.166133429054171\n",
      "The test loss is 0.6798302739359228\n",
      "The test loss is 0.9926217548261531\n",
      "The test loss is 1.1205532657825323\n",
      "The test loss is 0.9761096115277962\n",
      "The test loss is 0.7230453188218647\n",
      "The test loss is 0.8394622586870761\n",
      "The test loss is 1.0467013589539211\n",
      "The test loss is 1.058797153752119\n",
      "The test loss is 0.6731805400180622\n",
      "The test loss is 0.9060851364269943\n",
      "The test loss is 1.0117491747971536\n",
      "The test loss is 1.091023074193635\n",
      "The test loss is 0.0890852049881784\n",
      "The test loss is 0.021528680993335613\n",
      "The test loss is 0.04196226669324766\n",
      "The test loss is 0.025270552969465027\n",
      "The test loss is 0.0738911399330042\n",
      "The test loss is 0.11462433831490403\n",
      "The test loss is 0.0321442418138852\n",
      "The test loss is 0.03250696815903171\n",
      "The test loss is 0.7361315545372281\n",
      "The test loss is 0.5276207893927135\n",
      "The test loss is 0.058588425748491796\n",
      "The test loss is 0.02913156947428358\n",
      "The test loss is 0.08974323583421938\n",
      "The test loss is 0.03215128195803647\n",
      "The test loss is 0.05575199515360085\n",
      "The test loss is 0.019639003448056545\n",
      "The test loss is 0.02219293947894229\n",
      "The test loss is 0.023112816062560982\n",
      "The test loss is 0.020559971177353455\n",
      "The test loss is 0.01821644881535791\n",
      "The test loss is 0.020482275246399416\n",
      "The test loss is 0.019402178184023598\n",
      "The test loss is 0.018572901297003012\n",
      "The test loss is 0.021398648471074175\n",
      "The test loss is 0.026111788602132206\n",
      "The test loss is 0.02480961908564578\n",
      "The test loss is 0.02001003994633378\n",
      "The test loss is 0.022134551051736295\n",
      "The test loss is 0.026284442790552198\n",
      "The test loss is 0.02252219013559998\n",
      "The test loss is 0.017792158358351174\n",
      "The test loss is 0.021378447308462837\n",
      "The test loss is 0.030854469655260206\n",
      "The test loss is 0.03532958143799208\n",
      "The test loss is 0.07967183759870493\n",
      "The test loss is 0.05800988441068974\n",
      "The test loss is 0.026882011391009146\n",
      "The test loss is 0.022242675270670483\n",
      "The test loss is 0.051340481517164895\n",
      "The test loss is 0.05120102366661008\n",
      "The test loss is 0.028842411675729138\n",
      "The test loss is 0.02449298991031079\n",
      "The test loss is 0.03463466675240705\n",
      "The test loss is 0.06293214198696366\n",
      "The test loss is 0.021791185975929037\n",
      "The test loss is 0.03741824119959036\n",
      "The test loss is 0.042287478613697416\n",
      "The test loss is 0.06546223718976675\n",
      "The test loss is 0.7082427367515624\n",
      "The test loss is 0.8660686316358184\n",
      "The test loss is 1.0343498897330947\n",
      "The test loss is 1.0608854625075976\n",
      "The test loss is 0.7112888829854429\n",
      "The test loss is 0.9761218389253065\n",
      "The test loss is 1.176265162655746\n",
      "The test loss is 0.9788678247689965\n",
      "The test loss is 0.7201945043111996\n",
      "The test loss is 0.891017385555867\n",
      "The test loss is 1.1787917711731148\n",
      "The test loss is 1.1023481039827827\n",
      "The test loss is 0.5876243164844149\n",
      "The test loss is 1.0030896764604127\n",
      "The test loss is 0.9995998758932327\n",
      "The test loss is 1.0564365976949412\n",
      "iterations completed\n",
      "[128, 0.05, 0.001, 0.001, 32]\n",
      "0.014685474129954107\n"
     ]
    }
   ],
   "source": [
    "best_loss = 1\n",
    "best_params = [0] * 5\n",
    "\n",
    "for i in range(5, 8):\n",
    "    hidden_dim = 2**i\n",
    "    for k in range(-1, 2):\n",
    "        dropout = (2**k)*0.1\n",
    "        for l in range(2, 6):\n",
    "            learning_rate = 10**(-l)\n",
    "            for m in range(2, 6):\n",
    "                weight_decay = 10**(-l)\n",
    "                for n in range(2, 6):\n",
    "                    batch_size = 2**n\n",
    "                    trained_model = train(hidden_dim, 2, batch_size, learning_rate, weight_decay, 100, dropout)\n",
    "                    loss = test(trained_model, testing_data)\n",
    "                    if loss < best_loss:\n",
    "                        best_model = trained_model\n",
    "                        best_loss = loss\n",
    "                        best_params[0] = hidden_dim\n",
    "                        best_params[1] = dropout\n",
    "                        best_params[2] = learning_rate\n",
    "                        best_params[3] = weight_decay\n",
    "                        best_params[4] = batch_size\n",
    "\n",
    "\n",
    "torch.save(best_model.state_dict(),'simple_best.pth')                       \n",
    "print(\"iterations completed\")                        \n",
    "print(best_params)\n",
    "print(best_loss)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test loss is 0.014216222676088824\n",
      "0.014216222676088824\n"
     ]
    }
   ],
   "source": [
    "# input_dim, hidden_dim, output_dim, lstm_nums_layer, dropout\n",
    "best_model = SimpleLSTM(input_dim, 128,1, 2, 0.05)\n",
    "\n",
    "best_model.load_state_dict(torch.load('simple_best.pth'))\n",
    "best_model.to(torch.double)\n",
    "best_model.eval()\n",
    "print(test(best_model, testing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "yv9g1qAvzLUp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0545045927092933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/changyiyang/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning:\n",
      "\n",
      "Using a target size (torch.Size([234])) that is different to the input size (torch.Size([234, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "Real",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233
         ],
         "y": [
          1.01598,
          1.01509,
          1.01539,
          1.01502,
          1.01515,
          1.01477,
          1.01465,
          1.01426,
          1.01416,
          1.01405,
          1.01364,
          1.01395,
          1.01338,
          1.01355,
          1.01344,
          1.01321,
          1.01293,
          1.01274,
          1.01235,
          1.01232,
          1.01196,
          1.01212,
          1.01212,
          1.01202,
          1.01232,
          1.01242,
          1.01237,
          1.01236,
          1.01223,
          1.01228,
          1.01205,
          1.0121,
          1.01178,
          1.01197,
          1.01225,
          1.01261,
          1.01285,
          1.01317,
          1.01343,
          1.01383,
          1.01352,
          1.01328,
          1.01326,
          1.01293,
          1.01275,
          1.01311,
          1.01304,
          1.01275,
          1.01246,
          1.01235,
          1.01245,
          1.01282,
          1.01287,
          1.01333,
          1.01338,
          1.01371,
          1.01425,
          1.01441,
          1.01455,
          1.01477,
          1.01475,
          1.01535,
          1.01565,
          1.01595,
          1.01619,
          1.01647,
          1.01668,
          1.01747,
          1.01752,
          1.01769,
          1.01768,
          1.0179,
          1.01805,
          1.01817,
          1.01878,
          1.01863,
          1.01901,
          1.019,
          1.01908,
          1.01915,
          1.01897,
          1.01896,
          1.01902,
          1.01919,
          1.01943,
          1.01913,
          1.01888,
          1.01903,
          1.01934,
          1.01895,
          1.01874,
          1.01866,
          1.01855,
          1.01823,
          1.01816,
          1.01819,
          1.01816,
          1.01788,
          1.01742,
          1.01766,
          1.01745,
          1.01725,
          1.01688,
          1.01663,
          1.01644,
          1.01635,
          1.01645,
          1.01587,
          1.01586,
          1.01573,
          1.01554,
          1.01509,
          1.01512,
          1.01498,
          1.01491,
          1.01486,
          1.01451,
          1.01425,
          1.01402,
          1.01388,
          1.01385,
          1.01347,
          1.01381,
          1.01324,
          1.01296,
          1.01316,
          1.01289,
          1.01287,
          1.0126,
          1.01221,
          1.0122,
          1.01214,
          1.01234,
          1.0125,
          1.01223,
          1.01207,
          1.01211,
          1.01237,
          1.01233,
          1.01237,
          1.01209,
          1.01203,
          1.0121,
          1.01189,
          1.01232,
          1.01212,
          1.01267,
          1.01259,
          1.01257,
          1.01269,
          1.01288,
          1.01313,
          1.01304,
          1.01298,
          1.01308,
          1.01307,
          1.01292,
          1.0132,
          1.01306,
          1.013,
          1.01314,
          1.01316,
          1.01364,
          1.01375,
          1.01414,
          1.01423,
          1.01452,
          1.01441,
          1.01483,
          1.01533,
          1.01557,
          1.0155,
          1.01559,
          1.01591,
          1.01646,
          1.01675,
          1.017,
          1.017,
          1.01763,
          1.01735,
          1.0179,
          1.01797,
          1.01805,
          1.01828,
          1.01858,
          1.01814,
          1.01849,
          1.01878,
          1.0188,
          1.01889,
          1.01889,
          1.01865,
          1.01881,
          1.01872,
          1.01869,
          1.01876,
          1.01911,
          1.01899,
          1.0191,
          1.01888,
          1.0191,
          1.01888,
          1.01888,
          1.01858,
          1.01837,
          1.01823,
          1.01837,
          1.01811,
          1.01774,
          1.01758,
          1.01748,
          1.01732,
          1.01708,
          1.01698,
          1.01683,
          1.0167,
          1.01658,
          1.01627,
          1.01607,
          1.01587,
          1.01543,
          1.01526,
          1.01534,
          1.01511,
          1.01492,
          1.01455,
          1.01459,
          1.01423,
          1.01403,
          1.01389,
          1.01352,
          1.01335,
          1.0132,
          1.01319
         ]
        },
        {
         "mode": "lines",
         "name": "Predict",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233
         ],
         "y": [
          1.0157666486123937,
          1.0152133114139616,
          1.0153139320250404,
          1.0151480426344959,
          1.0151247582067984,
          1.014802617677966,
          1.0146028704042345,
          1.014195575165974,
          1.0142376437601146,
          1.0139374409298334,
          1.0134618349589006,
          1.0134993909330194,
          1.0134857821716199,
          1.0135799466318742,
          1.013301683714959,
          1.0131815975423923,
          1.0130696364730138,
          1.0128021828254326,
          1.0122686030948738,
          1.012436704837791,
          1.0117213998501702,
          1.0120509266585331,
          1.011791714422205,
          1.0121039501070699,
          1.012230682627413,
          1.0122664720441212,
          1.012380800480968,
          1.012372456248097,
          1.0122378503599496,
          1.0123031021155646,
          1.0118375303192901,
          1.011918303300306,
          1.0120041689185857,
          1.012064448233637,
          1.0123198754225422,
          1.0126225989986746,
          1.0129683090856527,
          1.0130583138104425,
          1.0136376962881628,
          1.013713972886153,
          1.013261600668915,
          1.0134340401988196,
          1.0131627293629597,
          1.0131902127108514,
          1.012944036641796,
          1.0130386913668012,
          1.0129902056135633,
          1.012859210114253,
          1.0124873120365805,
          1.0124485435463788,
          1.0123322475640057,
          1.0126436913009778,
          1.0131720836584304,
          1.0131961957334517,
          1.0135464769395588,
          1.0137271530063148,
          1.0142319450029444,
          1.014389131671154,
          1.0146810975187772,
          1.0147561384720587,
          1.0146443070927087,
          1.0151644500299344,
          1.0156667316327692,
          1.0159564243564552,
          1.0158907784270994,
          1.0164338989290336,
          1.01695785412248,
          1.0172633926917651,
          1.0176946599683927,
          1.017641592821929,
          1.01838314387845,
          1.018688697493698,
          1.0185004099384078,
          1.0185981962034516,
          1.0186249380515435,
          1.0188217546534997,
          1.0188825077299057,
          1.0189791966393122,
          1.0190670172736458,
          1.019372926180337,
          1.0199019116050547,
          1.019859229167931,
          1.0196632809631936,
          1.0194431559069674,
          1.0193050895182256,
          1.01919622903118,
          1.0193410532002147,
          1.0189113857105931,
          1.0191650728129882,
          1.0190641729020076,
          1.0196848731794665,
          1.0193881220259504,
          1.018835629739694,
          1.0184107939562075,
          1.0183380392822547,
          1.0182503298748975,
          1.0181057198756942,
          1.0178809069855945,
          1.0174388963694614,
          1.0178045977397905,
          1.0182886638787312,
          1.0180166395969577,
          1.0170400710244951,
          1.0167729607736933,
          1.0165043161081186,
          1.0163049859715902,
          1.016240220864841,
          1.0159982786107609,
          1.0159777710350886,
          1.0157690392089056,
          1.015665410393868,
          1.0150164344206583,
          1.0151867993488506,
          1.014852854668255,
          1.0149637243780072,
          1.014592343134926,
          1.0147096033562495,
          1.0139905789622776,
          1.0140088915181251,
          1.0139977605881128,
          1.0135285180452183,
          1.0132744636009987,
          1.0134419061229294,
          1.0131371206918598,
          1.0131141571848403,
          1.0132472051662988,
          1.0128065191993025,
          1.012916231358387,
          1.012557983611194,
          1.0122854028010244,
          1.011613828593192,
          1.0119567820591047,
          1.0122264098813292,
          1.0124197274453322,
          1.0121995083288475,
          1.012119712755138,
          1.0123462054797332,
          1.0123082670410997,
          1.012520319605701,
          1.012426366017386,
          1.0119538936148298,
          1.0118681534807188,
          1.012150364306177,
          1.0120275145730666,
          1.0123013945627402,
          1.0122864499233826,
          1.0126024645206664,
          1.0127524215688684,
          1.0125320264763058,
          1.0127220348049513,
          1.0125407616942423,
          1.0129753306979357,
          1.013087995543458,
          1.0131890601308273,
          1.013102395899434,
          1.013143207660085,
          1.0130013736003676,
          1.0131198004859443,
          1.013071748016857,
          1.0130552039512712,
          1.0125219403088053,
          1.0126901846280807,
          1.0135939263506342,
          1.013679303644349,
          1.0144963369793272,
          1.0141371518696323,
          1.0144314971117825,
          1.0144694805762413,
          1.0149737689790952,
          1.0151502489563742,
          1.0155830846147196,
          1.016084450797825,
          1.0159459736664807,
          1.0165371502290552,
          1.0160646126641146,
          1.0171286520645548,
          1.0173418670456467,
          1.017490647643024,
          1.0174616785287802,
          1.0177736620598132,
          1.0177906814487063,
          1.0178408240055894,
          1.018109288358647,
          1.0181536405461085,
          1.0183185291926793,
          1.0185716166860548,
          1.0189318104364637,
          1.0189297086042264,
          1.0188174075376812,
          1.0188393461767815,
          1.0188443961821083,
          1.019130624574341,
          1.0191372878302285,
          1.0188329296633252,
          1.0186764795664567,
          1.018596758819761,
          1.0185701463956789,
          1.0186634909238037,
          1.0188868306193202,
          1.0189227485051713,
          1.0185105208143166,
          1.018383770469636,
          1.0184056923927134,
          1.0184649687374454,
          1.0182473492140764,
          1.0180886806290252,
          1.0178199724556969,
          1.0174268885295963,
          1.0176862692831843,
          1.017660791888172,
          1.0178572244922717,
          1.017258041230509,
          1.0173965144756294,
          1.0170804073314272,
          1.0173187253485594,
          1.0170389228419905,
          1.0164272965660726,
          1.016342926506522,
          1.016101841396154,
          1.0158789246941116,
          1.0154291182949091,
          1.0156978964511383,
          1.015084974787062,
          1.0151411627127618,
          1.0144046827196995,
          1.0143576211236274,
          1.014503479996851,
          1.0146129708673446,
          1.014192822713823,
          1.013904439352287,
          1.01330046009013,
          1.0132638117343074,
          1.0136407985525506,
          1.0131955613644594
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "The real and predict Keff with model trained on stable state"
        },
        "xaxis": {
         "title": {
          "text": "Index"
         }
        },
        "yaxis": {
         "title": {
          "text": "Value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'stable_one.html'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn\n",
    "\n",
    "# Assuming 'best_model', 'data', and 'ss' (StandardScaler) are defined\n",
    "\n",
    "X = (torch.tensor(data[:, :-1])).double()\n",
    "y = (torch.tensor(data[:, -1])).double()\n",
    "\n",
    "y_pred = best_model(X)\n",
    "\n",
    "y_pred = y_pred.detach().numpy()\n",
    "\n",
    "X_pred = np.append(data[:, :-1], y_pred.reshape(-1, 1), axis=1)\n",
    "\n",
    "y_original = ss.inverse_transform(data)[:, -1]\n",
    "\n",
    "y_pred_original = ss.inverse_transform(X_pred)[:, -1]\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "print(loss(best_model(X), y).item())\n",
    "\n",
    "# Plotly graph\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(range(len(y_original))),\n",
    "                         y=y_original,\n",
    "                         mode='lines',\n",
    "                         name='Real'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(range(len(y_pred_original))),\n",
    "                         y=y_pred_original,\n",
    "                         mode='lines',\n",
    "                         name='Predict'))\n",
    "\n",
    "fig.update_layout(title='The real and predict Keff with model trained on stable state',\n",
    "                  xaxis_title='Index',\n",
    "                  yaxis_title='Value')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "import plotly.offline as pyo\n",
    "pyo.plot(fig, filename='stable_one.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0545045927092933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/changyiyang/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning:\n",
      "\n",
      "Using a target size (torch.Size([234])) that is different to the input size (torch.Size([234, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "Real",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233
         ],
         "y": [
          1.01598,
          1.01509,
          1.01539,
          1.01502,
          1.01515,
          1.01477,
          1.01465,
          1.01426,
          1.01416,
          1.01405,
          1.01364,
          1.01395,
          1.01338,
          1.01355,
          1.01344,
          1.01321,
          1.01293,
          1.01274,
          1.01235,
          1.01232,
          1.01196,
          1.01212,
          1.01212,
          1.01202,
          1.01232,
          1.01242,
          1.01237,
          1.01236,
          1.01223,
          1.01228,
          1.01205,
          1.0121,
          1.01178,
          1.01197,
          1.01225,
          1.01261,
          1.01285,
          1.01317,
          1.01343,
          1.01383,
          1.01352,
          1.01328,
          1.01326,
          1.01293,
          1.01275,
          1.01311,
          1.01304,
          1.01275,
          1.01246,
          1.01235,
          1.01245,
          1.01282,
          1.01287,
          1.01333,
          1.01338,
          1.01371,
          1.01425,
          1.01441,
          1.01455,
          1.01477,
          1.01475,
          1.01535,
          1.01565,
          1.01595,
          1.01619,
          1.01647,
          1.01668,
          1.01747,
          1.01752,
          1.01769,
          1.01768,
          1.0179,
          1.01805,
          1.01817,
          1.01878,
          1.01863,
          1.01901,
          1.019,
          1.01908,
          1.01915,
          1.01897,
          1.01896,
          1.01902,
          1.01919,
          1.01943,
          1.01913,
          1.01888,
          1.01903,
          1.01934,
          1.01895,
          1.01874,
          1.01866,
          1.01855,
          1.01823,
          1.01816,
          1.01819,
          1.01816,
          1.01788,
          1.01742,
          1.01766,
          1.01745,
          1.01725,
          1.01688,
          1.01663,
          1.01644,
          1.01635,
          1.01645,
          1.01587,
          1.01586,
          1.01573,
          1.01554,
          1.01509,
          1.01512,
          1.01498,
          1.01491,
          1.01486,
          1.01451,
          1.01425,
          1.01402,
          1.01388,
          1.01385,
          1.01347,
          1.01381,
          1.01324,
          1.01296,
          1.01316,
          1.01289,
          1.01287,
          1.0126,
          1.01221,
          1.0122,
          1.01214,
          1.01234,
          1.0125,
          1.01223,
          1.01207,
          1.01211,
          1.01237,
          1.01233,
          1.01237,
          1.01209,
          1.01203,
          1.0121,
          1.01189,
          1.01232,
          1.01212,
          1.01267,
          1.01259,
          1.01257,
          1.01269,
          1.01288,
          1.01313,
          1.01304,
          1.01298,
          1.01308,
          1.01307,
          1.01292,
          1.0132,
          1.01306,
          1.013,
          1.01314,
          1.01316,
          1.01364,
          1.01375,
          1.01414,
          1.01423,
          1.01452,
          1.01441,
          1.01483,
          1.01533,
          1.01557,
          1.0155,
          1.01559,
          1.01591,
          1.01646,
          1.01675,
          1.017,
          1.017,
          1.01763,
          1.01735,
          1.0179,
          1.01797,
          1.01805,
          1.01828,
          1.01858,
          1.01814,
          1.01849,
          1.01878,
          1.0188,
          1.01889,
          1.01889,
          1.01865,
          1.01881,
          1.01872,
          1.01869,
          1.01876,
          1.01911,
          1.01899,
          1.0191,
          1.01888,
          1.0191,
          1.01888,
          1.01888,
          1.01858,
          1.01837,
          1.01823,
          1.01837,
          1.01811,
          1.01774,
          1.01758,
          1.01748,
          1.01732,
          1.01708,
          1.01698,
          1.01683,
          1.0167,
          1.01658,
          1.01627,
          1.01607,
          1.01587,
          1.01543,
          1.01526,
          1.01534,
          1.01511,
          1.01492,
          1.01455,
          1.01459,
          1.01423,
          1.01403,
          1.01389,
          1.01352,
          1.01335,
          1.0132,
          1.01319
         ]
        },
        {
         "mode": "lines",
         "name": "Predict",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233
         ],
         "y": [
          1.0157666486123937,
          1.0152133114139616,
          1.0153139320250404,
          1.0151480426344959,
          1.0151247582067984,
          1.014802617677966,
          1.0146028704042345,
          1.014195575165974,
          1.0142376437601146,
          1.0139374409298334,
          1.0134618349589006,
          1.0134993909330194,
          1.0134857821716199,
          1.0135799466318742,
          1.013301683714959,
          1.0131815975423923,
          1.0130696364730138,
          1.0128021828254326,
          1.0122686030948738,
          1.012436704837791,
          1.0117213998501702,
          1.0120509266585331,
          1.011791714422205,
          1.0121039501070699,
          1.012230682627413,
          1.0122664720441212,
          1.012380800480968,
          1.012372456248097,
          1.0122378503599496,
          1.0123031021155646,
          1.0118375303192901,
          1.011918303300306,
          1.0120041689185857,
          1.012064448233637,
          1.0123198754225422,
          1.0126225989986746,
          1.0129683090856527,
          1.0130583138104425,
          1.0136376962881628,
          1.013713972886153,
          1.013261600668915,
          1.0134340401988196,
          1.0131627293629597,
          1.0131902127108514,
          1.012944036641796,
          1.0130386913668012,
          1.0129902056135633,
          1.012859210114253,
          1.0124873120365805,
          1.0124485435463788,
          1.0123322475640057,
          1.0126436913009778,
          1.0131720836584304,
          1.0131961957334517,
          1.0135464769395588,
          1.0137271530063148,
          1.0142319450029444,
          1.014389131671154,
          1.0146810975187772,
          1.0147561384720587,
          1.0146443070927087,
          1.0151644500299344,
          1.0156667316327692,
          1.0159564243564552,
          1.0158907784270994,
          1.0164338989290336,
          1.01695785412248,
          1.0172633926917651,
          1.0176946599683927,
          1.017641592821929,
          1.01838314387845,
          1.018688697493698,
          1.0185004099384078,
          1.0185981962034516,
          1.0186249380515435,
          1.0188217546534997,
          1.0188825077299057,
          1.0189791966393122,
          1.0190670172736458,
          1.019372926180337,
          1.0199019116050547,
          1.019859229167931,
          1.0196632809631936,
          1.0194431559069674,
          1.0193050895182256,
          1.01919622903118,
          1.0193410532002147,
          1.0189113857105931,
          1.0191650728129882,
          1.0190641729020076,
          1.0196848731794665,
          1.0193881220259504,
          1.018835629739694,
          1.0184107939562075,
          1.0183380392822547,
          1.0182503298748975,
          1.0181057198756942,
          1.0178809069855945,
          1.0174388963694614,
          1.0178045977397905,
          1.0182886638787312,
          1.0180166395969577,
          1.0170400710244951,
          1.0167729607736933,
          1.0165043161081186,
          1.0163049859715902,
          1.016240220864841,
          1.0159982786107609,
          1.0159777710350886,
          1.0157690392089056,
          1.015665410393868,
          1.0150164344206583,
          1.0151867993488506,
          1.014852854668255,
          1.0149637243780072,
          1.014592343134926,
          1.0147096033562495,
          1.0139905789622776,
          1.0140088915181251,
          1.0139977605881128,
          1.0135285180452183,
          1.0132744636009987,
          1.0134419061229294,
          1.0131371206918598,
          1.0131141571848403,
          1.0132472051662988,
          1.0128065191993025,
          1.012916231358387,
          1.012557983611194,
          1.0122854028010244,
          1.011613828593192,
          1.0119567820591047,
          1.0122264098813292,
          1.0124197274453322,
          1.0121995083288475,
          1.012119712755138,
          1.0123462054797332,
          1.0123082670410997,
          1.012520319605701,
          1.012426366017386,
          1.0119538936148298,
          1.0118681534807188,
          1.012150364306177,
          1.0120275145730666,
          1.0123013945627402,
          1.0122864499233826,
          1.0126024645206664,
          1.0127524215688684,
          1.0125320264763058,
          1.0127220348049513,
          1.0125407616942423,
          1.0129753306979357,
          1.013087995543458,
          1.0131890601308273,
          1.013102395899434,
          1.013143207660085,
          1.0130013736003676,
          1.0131198004859443,
          1.013071748016857,
          1.0130552039512712,
          1.0125219403088053,
          1.0126901846280807,
          1.0135939263506342,
          1.013679303644349,
          1.0144963369793272,
          1.0141371518696323,
          1.0144314971117825,
          1.0144694805762413,
          1.0149737689790952,
          1.0151502489563742,
          1.0155830846147196,
          1.016084450797825,
          1.0159459736664807,
          1.0165371502290552,
          1.0160646126641146,
          1.0171286520645548,
          1.0173418670456467,
          1.017490647643024,
          1.0174616785287802,
          1.0177736620598132,
          1.0177906814487063,
          1.0178408240055894,
          1.018109288358647,
          1.0181536405461085,
          1.0183185291926793,
          1.0185716166860548,
          1.0189318104364637,
          1.0189297086042264,
          1.0188174075376812,
          1.0188393461767815,
          1.0188443961821083,
          1.019130624574341,
          1.0191372878302285,
          1.0188329296633252,
          1.0186764795664567,
          1.018596758819761,
          1.0185701463956789,
          1.0186634909238037,
          1.0188868306193202,
          1.0189227485051713,
          1.0185105208143166,
          1.018383770469636,
          1.0184056923927134,
          1.0184649687374454,
          1.0182473492140764,
          1.0180886806290252,
          1.0178199724556969,
          1.0174268885295963,
          1.0176862692831843,
          1.017660791888172,
          1.0178572244922717,
          1.017258041230509,
          1.0173965144756294,
          1.0170804073314272,
          1.0173187253485594,
          1.0170389228419905,
          1.0164272965660726,
          1.016342926506522,
          1.016101841396154,
          1.0158789246941116,
          1.0154291182949091,
          1.0156978964511383,
          1.015084974787062,
          1.0151411627127618,
          1.0144046827196995,
          1.0143576211236274,
          1.014503479996851,
          1.0146129708673446,
          1.014192822713823,
          1.013904439352287,
          1.01330046009013,
          1.0132638117343074,
          1.0136407985525506,
          1.0131955613644594
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "arrowhead": 2,
          "ax": 0,
          "ay": -30,
          "showarrow": true,
          "text": "Train-Test Split",
          "x": 150,
          "xref": "x",
          "y": 1.0199019116050547,
          "yref": "y"
         }
        ],
        "shapes": [
         {
          "line": {
           "color": "rgba(255, 0, 0, 0.5)",
           "width": 2
          },
          "name": "Train Test Split",
          "type": "line",
          "x0": 150,
          "x1": 150,
          "xref": "x",
          "y0": 1.011613828593192,
          "y1": 1.0199019116050547,
          "yref": "y"
         },
         {
          "line": {
           "color": "rgba(0, 0, 0, 1)",
           "width": 2
          },
          "name": "Train Test Split",
          "type": "line",
          "x0": 150,
          "x1": 150,
          "xref": "x",
          "y0": 1.011613828593192,
          "y1": 1.0199019116050547,
          "yref": "y"
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "The Real and Predict Keff with Model Trained on Stable State"
        },
        "xaxis": {
         "title": {
          "text": "Index"
         }
        },
        "yaxis": {
         "title": {
          "text": "Keff"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn\n",
    "\n",
    "# Assuming 'best_model', 'data', and 'ss' (StandardScaler) are defined\n",
    "\n",
    "X = (torch.tensor(data[:, :-1])).double()\n",
    "y = (torch.tensor(data[:, -1])).double()\n",
    "\n",
    "y_pred = best_model(X)\n",
    "\n",
    "y_pred = y_pred.detach().numpy()\n",
    "\n",
    "X_pred = np.append(data[:, :-1], y_pred.reshape(-1, 1), axis=1)\n",
    "\n",
    "y_original = ss.inverse_transform(data)[:, -1]\n",
    "\n",
    "y_pred_original = ss.inverse_transform(X_pred)[:, -1]\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "print(loss(best_model(X), y).item())\n",
    "\n",
    "# Plotly graph\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(range(len(y_original))),\n",
    "                         y=y_original,\n",
    "                         mode='lines',\n",
    "                         name='Real'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(range(len(y_pred_original))),\n",
    "                         y=y_pred_original,\n",
    "                         mode='lines',\n",
    "                         name='Predict'))\n",
    "\n",
    "# Add a vertical line at x = 150\n",
    "fig.add_shape(type='line',\n",
    "              x0=150, x1=150,\n",
    "              y0=min(np.min(y_original), np.min(y_pred_original)),\n",
    "              y1=max(np.max(y_original), np.max(y_pred_original)),\n",
    "              yref='y',\n",
    "              xref='x',\n",
    "              line=dict(color='rgba(255, 0, 0, 0.5)', width=2),\n",
    "              name='Train Test Split')\n",
    "\n",
    "fig.update_layout(title='The Real and Predict Keff with Model Trained on Stable State',\n",
    "                  xaxis_title='Index',\n",
    "                  yaxis_title='Keff',\n",
    "                  annotations=[dict(x=150,\n",
    "                                   y=max(np.max(y_original), np.max(y_pred_original)),\n",
    "                                   xref=\"x\", yref=\"y\",\n",
    "                                   text=\"Train-Test Split\",\n",
    "                                   showarrow=True,\n",
    "                                   arrowhead=2,\n",
    "                                   ax=0,\n",
    "                                   ay=-30)])\n",
    "\n",
    "# Add a vertical line at x = 150\n",
    "fig.add_shape(type='line',\n",
    "              x0=150, x1=150,\n",
    "              y0=min(np.min(y_original), np.min(y_pred_original)),\n",
    "              y1=max(np.max(y_original), np.max(y_pred_original)),\n",
    "              yref='y',\n",
    "              xref='x',\n",
    "              line=dict(color='rgba(0, 0, 0, 1)', width=2),  # Change color to black\n",
    "              name='Train Test Split')\n",
    "\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "83d8af4ff1ccf0e43da2803a684c950e5c1874edeab0ee20d27ee23b5d3b3a9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
