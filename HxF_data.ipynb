{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is aimed to add Xe to the model and see if the model can be improved\n",
    "from src.DataPrepocessing import *\n",
    "from src.NeuralNets import *\n",
    "from src.TrainMethod import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Propocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part will generate a file with following parts\n",
    "* Keff\n",
    "* Power\n",
    "* Threshold\n",
    "* Histogramed Cs\n",
    "* Histogramed Xe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core_name = \"./HxF Data/90_ power/HxF_HTR10_90pow_core_state_targets.csv\"\n",
    "# HxF_name = \"./HxF Data/90_ power/HxF_HTR10_90pow_average_actual_concentrations_features.csv\"\n",
    "# output_name = 'HxF_90pow_offset_20_shuffle.csv'\n",
    "\n",
    "# core_name = \"./HxF data/110_ power/HxF_HTR10_110pow_core_state_targets.csv\"\n",
    "# HxF_name = \"./HxF data/110_ power/HxF_HTR10_110pow_average_actual_concentrations_features.csv\"\n",
    "# output_name = 'HxF_110pow_offset_20_shuffle.csv'\n",
    "\n",
    "core_name = \"./HxF data/Equilibrium (100_ power)/HxF_HTR10_equilibrium_core_state_targets.csv\"\n",
    "HxF_name = \"./HxF data/Equilibrium (100_ power)/HxF_HTR10_equilibrium_average_actual_concentrations_features.csv\"\n",
    "output_name = 'HxF_100pow_offset_20_shuffle.csv'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "offset = 20\n",
    "shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201, 2)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, get the keff power and threshold from tracked_results.csv\n",
    "\n",
    "core_file = pd.read_csv(core_name)\n",
    "core_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      Unnamed: 0      keff\n",
       "0             0  1.008524\n",
       "1             1  1.007775\n",
       "2             2  1.008039\n",
       "3             3  1.008851\n",
       "4             4  1.007800\n",
       "..          ...       ...\n",
       "196         196  1.006656\n",
       "197         197  1.006667\n",
       "198         198  1.007251\n",
       "199         199  1.007318\n",
       "200         200  1.007344\n",
       "\n",
       "[201 rows x 2 columns]>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_file.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201, 1)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_columns = [\"keff\"]\n",
    "core_data = core_file[extract_columns].to_numpy()\n",
    "core_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "HxF_data = pd.read_csv(HxF_name).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201, 321)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HxF_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the time align\n",
    "# although it does not have a clear effect on the result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "core_data = core_data[offset:]\n",
    "\n",
    "HxF_data = HxF_data[:-offset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181, 322)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "total_data = np.concatenate((core_data, HxF_data), axis=1)\n",
    "total_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00702068, 1.0059791 , 1.00685854, 1.00603427, 1.00622645,\n",
       "       1.00534578, 1.00684972, 1.00417747, 1.00546906, 1.00775532,\n",
       "       1.00651952, 1.00781142, 1.00744724, 1.00691805, 1.00675262,\n",
       "       1.00797133, 1.00851394, 1.00828078, 1.00793766, 1.00861885,\n",
       "       1.00803856, 1.00847591, 1.00768598, 1.00827627, 1.00893481,\n",
       "       1.00877258, 1.00768032, 1.00942144, 1.00805028, 1.00854249,\n",
       "       1.00975211, 1.00809828, 1.00844302, 1.00769571, 1.00901452,\n",
       "       1.00829102, 1.0079814 , 1.0079665 , 1.00765699, 1.00891185,\n",
       "       1.0078659 , 1.00827564, 1.00784611, 1.00825425, 1.00687009,\n",
       "       1.00809948, 1.0077374 , 1.00759215, 1.00818552, 1.00930937,\n",
       "       1.00948454, 1.00952284, 1.00740573, 1.00781907, 1.00718593,\n",
       "       1.007029  , 1.00719395, 1.00789884, 1.0068638 , 1.00854854,\n",
       "       1.00651696, 1.00705608, 1.00846512, 1.00820569, 1.00841725,\n",
       "       1.00759884, 1.00790105, 1.00742768, 1.00925967, 1.00757855,\n",
       "       1.00685205, 1.00725878, 1.00733224, 1.00706755, 1.00823556,\n",
       "       1.00758871, 1.00636243, 1.0073866 , 1.00784097, 1.00834868,\n",
       "       1.00811748, 1.00768755, 1.00800166, 1.00910804, 1.00886882,\n",
       "       1.00910439, 1.00613277, 1.00716616, 1.00737376, 1.00734245,\n",
       "       1.00588131, 1.00747358, 1.00716108, 1.00769228, 1.00682648,\n",
       "       1.00658405, 1.00688562, 1.00648228, 1.00707273, 1.00735475,\n",
       "       1.00804525, 1.00719428, 1.00745203, 1.00791425, 1.00790883,\n",
       "       1.00887859, 1.00816667, 1.00819245, 1.0088941 , 1.00870021,\n",
       "       1.00592015, 1.00769918, 1.0074028 , 1.00655249, 1.00654963,\n",
       "       1.0066357 , 1.00680703, 1.00736817, 1.00764643, 1.00645785,\n",
       "       1.00635348, 1.00761708, 1.00701048, 1.00778514, 1.00739193,\n",
       "       1.00763153, 1.00781582, 1.00823409, 1.00730146, 1.00841404,\n",
       "       1.00775111, 1.00769748, 1.00856385, 1.00852669, 1.00865759,\n",
       "       1.00854542, 1.00799175, 1.00653304, 1.00806235, 1.00717151,\n",
       "       1.00660039, 1.00842968, 1.00690364, 1.00776526, 1.00764463,\n",
       "       1.00743587, 1.00781898, 1.00695369, 1.00745461, 1.00808808,\n",
       "       1.00766069, 1.00766396, 1.00757688, 1.0086911 , 1.00788039,\n",
       "       1.00833276, 1.00648999, 1.00688964, 1.00812261, 1.00766031,\n",
       "       1.00826842, 1.0074406 , 1.00704262, 1.00804708, 1.00750569,\n",
       "       1.00784187, 1.00855645, 1.00693987, 1.00687002, 1.006734  ,\n",
       "       1.00625647, 1.007082  , 1.00632731, 1.0067765 , 1.00682895,\n",
       "       1.00510798, 1.00665587, 1.00666667, 1.00725081, 1.00731771,\n",
       "       1.00734394])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the first column is the keff\n",
    "\n",
    "total_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_ndarray(arr, chunk_size=10):\n",
    "    num_chunks = arr.shape[0] // chunk_size  # 计算块的数量\n",
    "\n",
    "    # 将数组切片为长度为 chunk_size 的块\n",
    "    chunks = np.split(arr[:num_chunks*chunk_size], num_chunks)\n",
    "\n",
    "    # 对块的索引进行洗牌（shuffle）\n",
    "    shuffled_indices = np.arange(num_chunks)\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "\n",
    "    # 根据洗牌后的索引重新排序块\n",
    "    shuffled_chunks = [chunks[i] for i in shuffled_indices]\n",
    "\n",
    "    # 将洗牌后的块重新合并为一个数组\n",
    "    shuffled_arr = np.concatenate(shuffled_chunks)\n",
    "\n",
    "    return shuffled_arr\n",
    "\n",
    "\n",
    "if shuffle:\n",
    "\n",
    "    total_data = shuffle_ndarray(total_data, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the data \n",
    "\n",
    "# Save the array as a CSV file\n",
    "np.savetxt(f'./HxF Data/{output_name}', total_data, delimiter=',')\n",
    "np.savetxt(f'./Processed Data/{output_name}', total_data, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83d8af4ff1ccf0e43da2803a684c950e5c1874edeab0ee20d27ee23b5d3b3a9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
