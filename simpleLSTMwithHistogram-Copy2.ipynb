{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xe7ZFAr0zLUl"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parameters() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5c6c08d2de3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Define the loss function and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: parameters() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Preprocess the data\n",
    "# Convert the count data into a time series with the appropriate number of time steps and features (bins)\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "# Define the LSTM model\n",
    "class model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size),\n",
    "                torch.zeros(1, batch_size, self.hidden_size))\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        mae = nn.L1Loss()(outputs, labels)\n",
    "        print('MAE: {:.4f}'.format(mae))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hX4XY0--zLUn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9b544cfcbf8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mReactorData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_percent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_percent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# skip the first line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# define the dataset classes\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "class ReactorData(Dataset):\n",
    "    def __init__(self,file_name, sequence_length, start_percent = 0, end_percent = 1):\n",
    "        data = pd.read_csv(file_name, skiprows=[0]) # skip the first line\n",
    "        \n",
    "        length = data.shape[0]\n",
    "        data = data[ int(length * start_percent)  : int(length * end_percent)]\n",
    "        \n",
    "        # print(data.shape)\n",
    "        \n",
    "        \n",
    "        self.labels = data.iloc[:, -1:]\n",
    "        self.data = data.iloc[:, 1:-1] # skip the first time column\n",
    "        \n",
    "        mm = MinMaxScaler()\n",
    "        ss = StandardScaler()\n",
    "\n",
    "\n",
    "        self.data = ss.fit_transform(self.data)\n",
    "        self.labels = mm.fit_transform(self.labels) \n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)//self.sequence_length\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        idx = idx * self.sequence_length\n",
    "        \n",
    "        return (torch.tensor(self.data[idx : idx+ self.sequence_length])).double(), \\\n",
    "    (torch.tensor(self.labels[idx : idx+ self.sequence_length])).double()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "p7MGcyurzLUn"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ReactorData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-41d909ecd3ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReactorData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fluence.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_percent\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_percent\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtesting_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReactorData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fluence.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_percent\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_percent\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ReactorData' is not defined"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "training_data = ReactorData('fluence.csv', sequence_length= 10, start_percent= 0, end_percent= 0.75)\n",
    "testing_data = ReactorData('fluence.csv', sequence_length= 10, start_percent= 0.75, end_percent= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.9933, -0.0986, -0.0830,  ..., -0.1448,  0.0000, -0.0830],\n",
      "        [ 5.5764,  2.4102, -0.0830,  ..., -0.1448,  0.0000, -0.0830],\n",
      "        [-2.7128, 11.7810, 12.0416,  ..., -0.1448,  0.0000, -0.0830],\n",
      "        ...,\n",
      "        [-1.8544, -0.0986, -0.0830,  ..., -0.1448,  0.0000, -0.0830],\n",
      "        [-1.5724, -0.0986, -0.0830,  ..., -0.1448,  0.0000, -0.0830],\n",
      "        [-1.4988, -0.0986, -0.0830,  ..., -0.1448,  0.0000, -0.0830]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(training_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AK4tjCHtzLUo"
   },
   "outputs": [],
   "source": [
    "# print(training_data[0][1])\n",
    "# print(training_data[0][0].shape)\n",
    "# print(training_data[0][1].shape)\n",
    "\n",
    "# print(len(training_data[0]))\n",
    "# print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RyAnlsI4zLUo"
   },
   "outputs": [],
   "source": [
    "# define the neural nets\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, lstm_nums_layer, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        \n",
    "        self.LSTM = nn.LSTM(input_dim, hidden_dim, lstm_nums_layer, batch_first = True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.hidden_to_output = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # print(input.shape)\n",
    "        \n",
    "        hidden_state, _ = self.LSTM(input)\n",
    "        \n",
    "        # print(hidden_state.shape)\n",
    "        output = self.dropout(hidden_state)\n",
    "        output = self.hidden_to_output(output)\n",
    "        \n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (2.11.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (23.3.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: packaging in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (20.9)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: setuptools in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.24.2)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.11.2)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.16.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/Vaibhav/opt/anaconda3/lib/python3.8/site-packages (from packaging->tensorflow) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fluence.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "X = df.values[:-1] # Input sequences\n",
    "y = df.values[1:] # Output sequences\n",
    "X_train, y_train = X[:100], y[:100] # Training set\n",
    "X_val, y_val = X[100:], y[100:] # Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "X = df.values[:-1] # Input sequences\n",
    "y = df.values[1:] # Output sequences\n",
    "X_train, y_train = X[:100], y[:100] # Training set\n",
    "X_val, y_val = X[100:], y[100:] # Validation set\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=64, verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_val)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "print('MSE: %.3f' % mse)\n",
    "\n",
    "# Make predictions\n",
    "X_test = np.array([df.values[-1]]) # Input sequence for prediction\n",
    "y_pred = model.predict(X_test)\n",
    "print('Predicted counts for next time step:', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9d-4dATzLUo"
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "input_dim = training_data[0][0].shape[1]\n",
    "output_dim = training_data[0][1].shape[1]\n",
    "\n",
    "\n",
    "# print(output_dim)\n",
    "\n",
    "# some adjustable hyper-parameters\n",
    "hidden_dim = 64\n",
    "num_hidden_layers = 1\n",
    "batch_size = 5\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "epoch_num = 200\n",
    "dropout = 0.2\n",
    "\n",
    "model = SimpleLSTM(input_dim, hidden_dim, output_dim, num_hidden_layers, dropout)\n",
    "model = model.double()\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size = batch_size)\n",
    "\n",
    "\n",
    "# the chosn loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay= weight_decay)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    \n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        \n",
    "\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        pred = model(X)\n",
    "        \n",
    "        # print(X.shape)\n",
    "        # print(y.shape)\n",
    "        \n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # backpropagation\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss = loss.item()\n",
    "        \n",
    "    if epoch % 5 == 0:\n",
    "        print(\"The loss is {} in epoch {}\".format(loss ,epoch))\n",
    "            \n",
    "\n",
    "print(f\"The training is ended, the final loss is {loss}.\")\n",
    "print(\"Bye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zahzhYfIzLUp"
   },
   "outputs": [],
   "source": [
    "# visualize the training output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X_train = (torch.tensor(training_data.data)).double()\n",
    "y_train = (torch.tensor(training_data.labels)).double()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(y_train, label = \"Real\")     \n",
    "plt.plot(model(X_train).detach().numpy(), label = \"Predict\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yv9g1qAvzLUp"
   },
   "outputs": [],
   "source": [
    "# visualize the test output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X_test = (torch.tensor(testing_data.data)).double()\n",
    "y_test = (torch.tensor(testing_data.labels)).double()\n",
    "\n",
    "y_pred = model(X_test)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "print(loss(y_pred, y_test).item())\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(y_test, label = \"Real\")     \n",
    "plt.plot(y_pred.detach().numpy(), label = \"Predict\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter search space\n",
    "from ray import tune\n",
    "\n",
    "config_space = {\n",
    "    \"hidden_size\": tune.choice([64, 128, 256]),\n",
    "    \"num_layers\": tune.choice([1, 2, 3]),\n",
    "    \"dropout\": tune.uniform(0.1, 0.5),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"num_epochs\": 10\n",
    "}\n",
    "\n",
    "# Launch the Ray Tune experiment\n",
    "analysis = tune.run(\n",
    "    model,\n",
    "    config=config_space,\n",
    "    num_samples=10,\n",
    "    search_alg=tune.suggest.hyperopt,\n",
    "    metric=\"mean_accuracy\",\n",
    "    mode=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tune.Tuner(\n",
    "    train_mnist,\n",
    "    param_space = {\n",
    "    'batch_size': tune.choice([5, 10, 20]),\n",
    "    'hidden_dim': tune.choice([32, 64, 128]),\n",
    "    'num_hidden_layers': tune.choice([1, 2, 3, 4, 5]),\n",
    "    'learning_rate': tune.choice([1e-6, 1e-5, 1e-4, 1e-3]),\n",
    "    'weight_decay': tune.choice([1e-6, 1e-5, 1e-4, 1e-3]),\n",
    "    'epoch_num': tune.choice([50, 100, 120, 150, 180, 200, 400]) },\n",
    "    tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\")\n",
    ")\n",
    "analysis = tuner.fit()\n",
    "print(\"Best config: \", analysis.get_best_result(\"mean_loss\",\"min\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "83d8af4ff1ccf0e43da2803a684c950e5c1874edeab0ee20d27ee23b5d3b3a9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
